{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ahj1mVNURSD"
      },
      "outputs": [],
      "source": [
        "!pip install youtube-transcript-api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import googleapiclient.discovery\n",
        "\n",
        "API_KEY = \"AIzaSyC1wCUoKeNvw3t6ptpuZYogRdvcjtc7Rms\"  # Replace with your valid YouTube Data API key\n",
        "\n",
        "# Define the channel ID\n",
        "channel_id = \"UCfSqNB0yh99yuG4p4nzjPOA\"  # Replace with the desired channel ID\n",
        "\n",
        "def get_video_ids(channel_id):\n",
        "  \"\"\"\n",
        "  Gets a list of video IDs from a YouTube channel.\n",
        "\n",
        "  Args:\n",
        "    channel_id: The ID of the YouTube channel.\n",
        "\n",
        "  Returns:\n",
        "    A list of video IDs.\n",
        "  \"\"\"\n",
        "  youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
        "\n",
        "  # Get the channel's upload playlist ID\n",
        "  request = youtube.channels().list(\n",
        "      part=\"contentDetails\",\n",
        "      id=channel_id,\n",
        "  )\n",
        "  response = request.execute()\n",
        "  print(response)\n",
        "  playlist_id = response[\"items\"][0][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
        "\n",
        "  # Get videos from the upload playlist\n",
        "  video_ids = []\n",
        "  next_page_token = None\n",
        "  while True:\n",
        "    request = youtube.playlistItems().list(\n",
        "        part=\"snippet\",\n",
        "        playlistId=playlist_id,\n",
        "        maxResults=50,\n",
        "        pageToken=next_page_token,\n",
        "    )\n",
        "    response = request.execute()\n",
        "\n",
        "    for item in response[\"items\"]:\n",
        "      video_id = item[\"snippet\"][\"resourceId\"][\"videoId\"]\n",
        "      video_ids.append(video_id)\n",
        "\n",
        "    next_page_token = response.get(\"nextPageToken\")\n",
        "    if not next_page_token:\n",
        "      break\n",
        "\n",
        "  return video_ids\n",
        "\n",
        "# Get the video IDs\n",
        "video_ids = get_video_ids(channel_id)\n",
        "\n",
        "# Print the video IDs\n",
        "print(f\"Video IDs for channel {channel_id}:\")\n",
        "for video_id in video_ids:\n",
        "  print(f\"- {video_id}\")\n"
      ],
      "metadata": {
        "id": "0i3pMx62bJq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "transcripts = []\n",
        "\n",
        "for video_id in video_ids:\n",
        "\n",
        "  text = \"\"\n",
        "\n",
        "  # Experiment: last video\n",
        "  try:\n",
        "\n",
        "    # Get the transcript\n",
        "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "\n",
        "    # Append all the entries to a single text. Add newlines between different\n",
        "    # parts to make post-processing easier later.\n",
        "    for entry in transcript:\n",
        "\n",
        "      # Remove certain unnecessary words and get the text we need to retain.\n",
        "      txt = entry['text'].replace(\"uh\", \"\")\n",
        "      txt = txt.replace(\"um\", \"\")\n",
        "\n",
        "      text += (\" \" + txt)\n",
        "\n",
        "    print(len(text))\n",
        "\n",
        "    # Append to our list of transcripts.\n",
        "    transcripts.append(text)\n",
        "\n",
        "  # Ignore failures.\n",
        "  except Exception as e:\n",
        "    print(\"Some error happened. Skipping this transcript\")\n"
      ],
      "metadata": {
        "id": "2r3wYLY1UnH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcripts[5][2000:3000]"
      ],
      "metadata": {
        "id": "nRAp9u1sSKe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The text comes in quite an unprocessed way.\n",
        "# It is really difficult to be accurate in separating it into sentences,\n",
        "# so we can use this library to get somewhat of an approximation!\n",
        "\n",
        "import spacy\n",
        "\n",
        "def process_transcript(i):\n",
        "\n",
        "  text = transcripts[i]\n",
        "\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc = nlp(text)\n",
        "\n",
        "  # Get the list of sentences.\n",
        "  sentences = [sentence.text + \".\" for sentence in doc.sents]\n",
        "  print (len(sentences))\n",
        "\n",
        "  # Now we need to pair up shorter sentences in order to minimize the number of\n",
        "  # times we'll need to call our model.\n",
        "  sentences_post = []\n",
        "\n",
        "  i = 0\n",
        "  while i < len(sentences):\n",
        "\n",
        "    s = \"\"\n",
        "\n",
        "    while i < len(sentences):\n",
        "\n",
        "      if len(s) + len(sentences[i]) >= 400:\n",
        "\n",
        "        if s == \"\":\n",
        "          i = i + 1\n",
        "\n",
        "        break\n",
        "\n",
        "      s += (sentences[i] + \" \")\n",
        "      i += 1\n",
        "\n",
        "    # Only keep large sentences.\n",
        "    if s != \"\" and len(s) >= 250:\n",
        "      sentences_post.append(s)\n",
        "\n",
        "  return sentences_post\n",
        "\n",
        "  # for sent in sentences_post:\n",
        "  #   print(f\"{len(sent)} -> {sent}\")\n"
      ],
      "metadata": {
        "id": "-9CK2JCPcENo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def get_summary(i):\n",
        "\n",
        "  # IDEA 1: try to append current summary with next sentence to summarize the entire lecture.\n",
        "  # VERDICT: This doesn't work very well. The model just keeps previous information and forgets\n",
        "  #          about new one.\n",
        "\n",
        "  # IDEA 2: Summarize in layers. First summarize every sentence with small sentences,\n",
        "  #         then summarize batches of those and so on.\n",
        "\n",
        "  sentences_post = process_transcript(i)\n",
        "\n",
        "  generator = pipeline('summarization',\n",
        "                        model='t5-small',\n",
        "                        max_length=50)\n",
        "\n",
        "  summaries = []\n",
        "\n",
        "  i = 0\n",
        "  for sent in sentences_post:\n",
        "\n",
        "    if i % 5 == 0:\n",
        "      print(f\"Progress: {i/float(len(sentences_post))*100}%\")\n",
        "\n",
        "    summaries.append(generator(\"summarize: \" + sent)[0][\"summary_text\"])\n",
        "\n",
        "    i += 1\n",
        "\n",
        "  print(\"Stage 1: DONE\")\n",
        "\n",
        "  summaries2 = []\n",
        "\n",
        "  j = 0\n",
        "  while j < len(summaries):\n",
        "\n",
        "    if j + 2 < len(summaries):\n",
        "      sent = summaries[j] + \" \" + summaries[j + 1] + summaries[j + 2]\n",
        "    elif j + 1 < len(summaries):\n",
        "      sent = summaries[j] + \" \" + summaries[j + 1]\n",
        "    else:\n",
        "      sent = summaries[j]\n",
        "\n",
        "    summaries2.append(generator(\"summarize: \" + sent)[0][\"summary_text\"])\n",
        "\n",
        "    j += 3\n",
        "\n",
        "  print(\"Stage 2: DONE\")\n",
        "\n",
        "  generator = pipeline('summarization',\n",
        "                        model='t5-small',\n",
        "                        max_length=30)\n",
        "\n",
        "  summary = \"\"\n",
        "\n",
        "  j = 0\n",
        "  while j < len(summaries2):\n",
        "\n",
        "    if j + 2 < len(summaries2):\n",
        "      sent = summaries2[j] + \" \" + summaries2[j + 1] + summaries2[j + 2]\n",
        "    elif j + 1 < len(summaries2):\n",
        "      sent = summaries2[j] + \" \" + summaries2[j + 1]\n",
        "    else:\n",
        "      sent = summaries2[j]\n",
        "\n",
        "    summary += (generator(\"summarize: \" + sent)[0][\"summary_text\"] + \"\")\n",
        "\n",
        "    j += 3\n",
        "\n",
        "  print(\"DONE\")\n",
        "  return summary\n"
      ],
      "metadata": {
        "id": "DUb1OPCUXi8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = []\n",
        "for i in range(len(transcripts)):\n",
        "  print(f\"Transcript: {i}\")\n",
        "  summary = get_summary(i)\n",
        "\n",
        "  # Append summary to file.\n",
        "  with open(\"./summaries.txt\", \"a\") as file:\n",
        "    file.write(f\"Lecture {i}\\n\" + summary + \"\\n\\n\")"
      ],
      "metadata": {
        "id": "TGNa6uYLuL3e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}