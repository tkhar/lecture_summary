Lecture 0
log Mel is the decb is the fact that the loudness is pitch . han with psycho Acoustics says the brain process isit becomes love it hard it almostpitch F0 the fundamental frequency is kind of your natural frequency you know some mostly women have higher voices andthis is the pitch track it's generated by Adobe aition . he found it difficult and went down in Pitch .8% of people putting in in there KN . but it doesn't make sense to me . a lot down inwhisper is an open AI product or model and you can download it . it's a good example of the Transformer phase and they'the review session will be held on thursday . right sample I think tried to give you a good indication the on Tuesday I

Lecture 1
i need sated running like you I can't like focusing out think about my like down too mul autopilot .he talked to his friend who's my good friend his brother and he told me no he didn't think this is reallygoogle believes that if they give you a surrounding that's creative you will also be creative . you will take the time to connectit's really being very smart about which pieces of the docent we can get to prompt engine talk for a second about so there'you can ask you're going to find that Google will provide even more endpoints that are available in the cloud that you can go init's pretty clear that what you're getting behavior is is the basis of moor components which is a b bc

Lecture 2
autopilot autopilot is like too much like it's dangerous to go aut . 93-year-old enj needshe talked to his friend who's my good friend his brother and he told me no he didn't think this is reallygoogle believes that if they give you a surrounding that's creative you will also be creative . you will take the time to connectit's really being very smart about which pieces of the docent we can get to prompt engine talk for a second about so there'you can ask you're going to find that Google will provide even more endpoints that are available in the cloud that you can go init's pretty clear that what you're getting behavior is is the basis of moor components which is a b bc

Lecture 3
last time we talked about we're in talking about the Transformer family talked about Bert . oh that'll be clever let methis the family tree here is an evolutionary tree I don't know if you can see down here it starts with embeddings herethis is sometimes called zero shot learning you've heard of f shot fuse shot is zero shot is it can do things it's nevertechnical manuals I wrote once I wonder if it's in there there may I should them but just a ton more stuff rightdiagnostics is recognizing textual entailment you have it's like your SAT test or whatever you have some statement of somethingbuilding a knowledge graph or a relationship graph so you can rank the a competition to see which sentences do the best at getting lowle

Lecture 4
last time we talked about we're in talking about the Transformer family talked about Bert . I'll have office hours to try to keepthis the family tree here is an evolutionary tree I don't know if you can see down here it starts with embeddings herethis is sometimes called zero shot learning you've heard of f shot fuse shot is zero shot is it can do things it's nevertechnical manuals I wrote once I wonder if it's in there there may I should them but just a ton more stuff rightso you know you you you spend 10 million $12 million TR of computing power training it on that . if you're a computerwe do have things like W Alpha that do a brilliant job of doing manipulation so I'm sure they're trying to integrate .

Lecture 5
we're going to have a presentation from a new professor Nong Kim . she interviewed her position in linguistics and shelike this this big function can't always be abstracted away like not of model evaluation can be model agnostic .so I think she's great and please sign up to the class spots are limited they're going fast . the original encoder dethe student learns to imitate the teacher it doesn't so much as it's given the same data . think of an apprenticethe contextual EMB sets are already doing something like that right there's a lot of tokens available in this system they give it entireyou can now add different heads to it to do various kinds of tasks so you can do sentence classification with it now remember that . you canlet me say the sample size is five Contin this with soes basing it on the Republic then the X the input would bethe lstm network is learning from the start symbol to the end symbol . the padding is sort of ignored now here'sPOS Sugg I'm going to try but L said that was a very good point you can ignore the they just which means have

Lecture 6
we're going to have a presentation from a new professor Nong Kim . she interviewed her position in linguistics and shethere are no Hub units attached to it . you don't whatever I'm sorry but whatever Hub units are attached you will have alreadyso I think she's great and please sign up to the class spots are limited they're going fast . the original encoder dethe student learns to imitate the teacher it doesn't so much as it's given the same data . think of an apprenticethe contextual EMB sets are already doing something like that right there's a lot of tokens available in this system they give it entireyou can now add different heads to it to do various kinds of tasks so you can do sentence classification with it now remember that . you canlet me say the sample size is five Contin this with soes basing it on the Republic then the X the input would bethe lstm network is learning from the start symbol to the end symbol . the padding is sort of ignored now here'sPOS Sugg I'm going to try but that was a very good point you can ignore the they just which means have all'

Lecture 7
I'm happy to jp on Zoom my partner Liz maybe less enthusiastic about me jping on Zoom multiple times a day the choice function which you've been using is on the website . you give the probability distribution it goes through and makes a random Choicethis is the same [Music] proportions as that except now you can give it to your choice algorithm so just think of it as isolate thethere's 512 of these going through this . this is an enormously complicated design the head the critical thing is the is this unitif the sequence is length 512 you're getting a one HW out . it's a probability distribution just likethe you know that they have basically transformed the data and now we're using a more subtle model you can here's your data youit's the input sequence and then decoding or encoding . they add this idea that you know you put in start symbol it's just learning that as soon as it gets to this it just repeats they all have to be the same length for a it's just like like the whole family baby you know grandkids everybody . they constantly get everybody's constantly my my I

Lecture 8
I'm happy to jp on Zoom my partner Liz maybe less enthusiastic about me jping on Zoom multiple times a day the choice function which you've been using is on the website . you give the probability distribution it goes through and makes a random Choicethis is the same [Music] proportions as that except now you can give it to your choice algorithm so just think of it as isolate thethere's 512 of these going through this . this is an enormously complicated design the head the critical thing is the is this unitif the sequence is length 512 you're getting a one HW out . it's a probability distribution just likethe you know that they have basically transformed the data and now we're using a more subtle model you can here's your data youit's the input sequence and then decoding or encoding . they add this idea that you know you put in start symbol it's just learning that as soon as it gets to this it just repeats they all have to be the same length for a it's just like like the whole family baby you know grandkids everybody . they don't believe in science either nobody will

Lecture 9
ph19 has a 34 layer network for eight 16 17 . it's left residual left from an earlier stage and just getswhen you get the third one you have this right you have everything you had here and there's all these different pads which put these linear layersBrian in our department working on very specific problems he has a certain design . he tweaks it over time, and hethe first thing that would occur to you is simply indices so here instead of adding them I have aggregated them by just concatenyou get the positional embedding of which word 0 1 2 and you add them . right you would get it if weyou've got to basically do this 512 is the width for one input but it sounds like you're duplicating the same thingit is designed to do this and when they look at this probability distribution it is telling you the relationship between this word and itself . it'if you look at lyrics and they correspond to chords now you could do them separately . but now there's a lot ofsheer Terror can happen every once in a while . it's a thousand hours of sheer boredom and 30 seconds .

Lecture 10
ph19 has a 34 layer network for eight 16 17 . it's left residual left from an earlier stage and just getswhen you get the third one you have this right you have everything you had here . there's all these different pad which put these linearBrian in our department working on very specific problems he has a certain design . he tweaks it over time, and hethe first thing that would occur to you is simply indices so here instead of adding them I have aggregated them by just concatenthe dimension is four you add it to 0101 . it makes sense you're combining two embeddings here they'reyou've got to basically do this 512 is the width for one input but it sounds like you're duplicating the same thingeight of them into eight parallelthe output is a vector Neal Network so like gu location of the machine of the daily like data aboutif you look at lyrics and they correspond to chords now you could do them separately . but now there's a lot ofsheer Terror can happen every once in a while . it's a thousand hours of sheer boredom and 30 seconds .

Lecture 11
BL blah they actually form part of your network we'll talk about that . there aren't a lot ofconvolution and pooling layers often get small you have fully connected . a typical thing in at least in fully connected layer is to getembedding has the dimensions have more meaning for peptides and they do for words . but I tried to read it .what does get do you can use get with any kind of dictionary . if the word is in the dictionary it returns it if itseparated two neurons should be able to do it will do as good a job as more neurons . but you can see that there'

Lecture 12
cross validation is an alternative to the static strategy we used in the last homework . but we'll think about that it's not naturallysome of you probably saw this at some point you did this what happened you ran it for 10,000 epics and then you change something and you pickle doesn't seem to have some update it needs to be . pick foring can't function . the list of youyou're going to dynamically choose pieces of the training set for validation . it's typical to use five to 10 somewhere in theredropout the main problem we're trying to address is overfitting . we want this network to learn the information that's inI promise I promise but it works well in a network unlike a classroom or a company and it's exceedingly simple .training loss is always if you did everything right going to go down . it will go up as it overfits because you'reit's all done to parallel it speeds it up not a not not by a of 64 necessar met usage 

Lecture 13
cross validation is an alternative to the static strategy we used in the last homework . but we'll think about that it's not naturallysome of you probably saw this at some point you did this what happened you ran it for 10,000 epics and then you changed something and you a lot of people use pickle is there an advantage to pickle . the list of you know huge matrices of parametersyou're going to dynamically choose pieces of the training set for validation . it's typical to use five to 10 somewhere in theredropout the main problem we're trying to address is overfitting . we want this network to learn the information that's inI promise I promise but it works well in a network unlike a classroom or a company and it's exceedingly simple .training loss is always if you did everything right going to go down . it will go up as it overfits because you'reit's all done to parallel it speeds it up not a not not by 64 necessar met usage . I usually

Lecture 14
I asked people in the department what materials they had in Transformers Soo . they gave me a slide 100 slides so I'mif you're trying to characterize what words are important really really common words are not important because they're in all docentsyou start training if you don't rerun the model when you create the model it initializes it with random parameters then itwe then refined it by using docent frequencies making words using the idea of how often they occur in different docents . now we havethe adjective is postp the same and then directions forward and backward . oh yeah theal phrases but in other languages German andthis is unidirectional it's a decoder unroll and the first you see the zero up there my arm is the zero

Lecture 15
I asked people in the department what materials they had in Transformers Soo . they gave me a slide 100 slides so I'mif you're trying to characterize what words are important really really common words are not important because they're in all docentsyou start training if you don't rerun the model when you create the model it initializes it with random parameters then itwe started with the simplest one bag of words we're stuck in . the same as bag of word . we then refined itadjectives are postponed right and so in french in romance langu usually this was a in Latin adjective can be anywhere but we're going to make that transition more subtle and allow more interaction between the two but in a controlled way the critical thing here folks is

Lecture 16
Kathy go Martin, from first time known for a long time, works in political science . she uses natural process to do some of thewe'll keep candidates in a candidate cue it's going to be a priority cue . n is you'go back and talk about word edings in a little more detail . the idea is in some sense make sense and youin a skip gr you have context on both sides so here's a five a window of five words almost always you have inarbitrary you can make it as wide or as narrow as you want . the longer you train it the more accurately it will approximate the rowthe embeddings colns are the ined in either one of these incredibly easy to get them know . here youthe show performance curve was put together two weeks ago . I think everything sh the training validation and the validation accurac and then and

Lecture 17
I'll put a note on the P tomorrow at 12 in CDs FL a friend of mine from first time known for awe'll keep candidates in a candidate cue it's going to be a priority cue . n is you'right Welch wanted to know welch was worried individual help best individual differences . the winner is Welching was wild with theinput predict an outut now the first one is Skip GRS . in the continuous bag of words approach you predict the target word frommonae is a row of the cooccurrence Matrix computers going to be very unlikely computers . it doesn't happen inthe data and the label it's a multi-hot vector and a one hot vector which is the word you know . sotwo approaches one is to invent a unique token called unknown every word gets replaced with unknown . we talked about this before with smoothing

Lecture 18
we're going to start to look at the diagrams in the next few weeks . it's solved in exactly the same way thatif you think about what's happening right here for whatever this word is you pass it to so you've got information from both sidesin the normal case we input a whatever we input it went through . we compare the label with the actual label the true . logcross entity is the same thing as we did before you would use cross entity . it chooses a tag there's an finitefirst president of the u.s. was born into slavery in the farm of no no . ambiguities are common in the speechwhen you use it as a generator of course you give it this heart symbol . but it automatically feeds it to the next level righthan annotated corpora is a canonical translation that uses han measures last time time . it's the

Lecture 19
we're going to start to look at the diagrams in the next few weeks . it's solved in exactly the same way thatif you think about what's happening right here for whatever this word is you pass it to so you've got information from both sidesin the normal case we input a whatever we input it went through . we compare the label with the actual label the true . logcross entity is the same thing as we did before you would use cross entity . it chooses a tag there's an finitefirst president of the u.s. was born into slavery in the farm of no no . ambiguities are common in the speechwhen you use it as a generator of course you give it this heart symbol . but it automatically feeds it to the next level righthan annotated corpora is a canonical translation that uses han measures last time time . it's the

Lecture 20
we're going to talk about getting into the really cool stuff for current networks . don't mind I have yeah completely the building youthe cycle blah is the cycle of french . because of where I grew up I'm fluent in French not German Frenchthe activation here from the previous step going to go out will go to get G it around it's going to get around like this this is the same neuron at different moments in time in the sequence there a for Loop controlling process . a lot of different configurationit's an example of this give it a vector have it generate a Sate some a nice example is image captioning it can be done with advantages in any network is to normalize between layers so you have a layer and the information goes through the layer nowwe used to do twitter and then no you can get a lot of data it's really hard to say see I've never done

Lecture 21
we're going to talk about getting into good stuff for current Networks . so just to show you where we are going . okaythe cycle blah is the cycle of french . because of where I grew up I'm fluent in French not German Frenchthe activation here from the previous step going to go out will go to get G it around it's going to get around like this this is the same neuron at different moments in time in the sequence there a for Loop controlling process . a lot of different configurationit's an example of this give it a vector have it generate a Sate some a nice example is image captioning you know for months companies train their products for months months and months on very large parallel systems that's first problem is that you have just a partner who's kind of immune compromise doesn't have we used to do twitter and then no you can get a lot ofe. [Music] spe I got your so look for it . if it doesn't turn out give youit .

Lecture 22
you should watch the whole thing if you're not familiar with pytorch . at the end basically end of theif you copy them right here notice what I've done here data directory the input file is the data directory here's the DATthe model by the way is a dictionary right doesn't work because it's not a string . the 19th that you

Lecture 23
this is the walkthrough video for homework 4 I may break it up in two pieces and concatenate them . in any case thecoln is the string of all the lines in the play concatenated together . coln has 36 rows and one coln this is the order in the file so just make sure that you keep the plays in the same order now if you make the keys and theglove which is pre-trained on 400,000 words and a lot of text so the idea here is that you will create a wordthis is now going to be symmetric again if you do with a dictionary . for every word you have a nested

Lecture 24
I gave the lecture on pytorch and gave you some sample code . you'll have another assignment tomorrow probably in theyou just flatten it just you know like I talked about last time time . that goes through 728 inputs and that's goingrecall you want to balance the two . it's the harmonic mean like we did in previous cases in this class or the perplexityin the mist digit recognition task you know the episodes of your life the immense amount of information that's come in is not stored theremodel is optimal in terms of how it did on the validation which is your health outset that should tell you how well you're doing let me know if you have any questions anything you want to talk about projects . we'll talk about what your interests are okay 

Lecture 25
tensors are the data structure primary data structure used in pytorch and this notebook I keep I don'tit is going to take Co the rows are going to be zero and one . you can also go torch Dot and have the functional versionyou've got to specify it here when you create it or cast it . one way to do it is to use two wait athe colns are the features here being X and Y . so that's the data a th rows each examplethe idea is that this is supposed to give you a nber telling you how likely it is to be in the zeroth group onef is a transform meaning it's one of the functions used to transform data from torch Vision just that's boiler plate .thistqdm is a nice way to see how much progress you're making . it's a really difficult thinglet me try let me do this test . it gave me these out and look look how big these nbers are wow rightit's unusual with real data to get 100% 99.5 is pretty darn good . but could I have done it with less epicsthe more epics the better but sort of maybe let's see what happens here . this is weird here we're tracking loss togetheryou do not want to run one samp through at a time for lots of reasons . it takes the average of all the placesin pytorch we have what's called a data loader . it'll automatically give you batches as ityou know the testing is supposed to be independent like another course not this one where you don't have a chance after that that'sthe loss will come out and you'll just access it so you just have to make sure that whenever you use the loader the data thata lot of this is canonical right doing the training and the testing this is pretty standard . if you have a large

Lecture 26
if you're a holiday indigenous people, go to a virtual Monday . it's a Monday so just to remindthe back propagation was vented or stabilized until the 70s late 70s . the hardware we have due to all the gamers whob is a weight but there's no input and I'm going to show you a trick for how to do that inneuron can learn a function that can be scaled into different places similar to moving the line so it's absolutely necessary . linear algebra is going to implement everything like this . there's a lot of chance for parallelism in notationally to lookit's the most videographed city in the world because of all the terrorism anyway the classes okay are just labels . so8 dog 7 doesn't need to add up to one . 80% of the dog 7 adds up to a threshold so it takes any sequence of nbers and performs the analogous operation to the sigma . it does it for you multiply one times the log of the other and remember this is between zero and one . it's the log is going to be negative

Lecture 27
if it's longer than that then replace that with count how many times the whole corpus is fine the rest of it is fine Loosely inspired by the brain or by neurons in the brain in the early days . they tried to model this a littlewe didn't emphasize that what you're doing is multiplying it by some floating Point nber like X . ifneuron can learn a function that can be scaled into different places similar to moving the line . if you always have athis is what is learned this is a set of parameters when we talk about you know a 200 billion parameter model it's two billionwe have the ability to have a set of inputs and to send it through these layers and a vector comes out the other it'you have at least two you have a distribution and you have to choose one so which one do you choose the highest one . 80%you're going to sup all of for the whole sequence the denominator here is the same for each one of them oh no the floating points they normalized between 0 and one and 28 by 28 . so what you do is you flattenwhy didn't you do the other way I did oh then normalize isn't the problem let's talk outside let

Lecture 28
so I'm gonna do a zoom lecture unless some cataclysm happens . so there's no classmodel is going to predict that the output should be y hat . but sometimes you see it the other way around doesn't matterthe first thing you might think of doing if you wanted to use regression would be have the x-axis height in inches and thentransform in mathematics you transform into another domain where you can do something useful . then you transform it back so in this one the transform to goit's an iterative approximation alb searching for the answer . there's a whole bunch of these in differentyou want to find the absolute minim that would be the best possible model you could get now it may not ever be zero . there may beit's going to tell me how far to jp down . then I'm going to subtract it because I want to gospam great news when they misspell it it's not from your bank right if you're wondering if something is Spam or

Lecture 29
so I'm going to do it's the only lecture I'll do on Zoom unless some cataclysm happenswe Square them could take the out value squaring is is equivalent to a distance function . if it's below thethis isn't a terribly good what I want is a probability . you can see if the probability goes from zerowe're going to say is this spam or not spam you can get a probability maybe that's good enough or 01. ifyou multiply with your set of Weights that you're learning apply the sigo or theof whatever that's how it functions .square of the mean square error you get a parabola or paraboloid in multiple dimensions . square them add them up divide by

Lecture 30
if I stay on this side audio is better so I'll ask some about I tend to wander around a lot during the leuregovernment and news and editor will be closer it's a really interesting way to look at the data I'm a little suspicious ofwhere is from the mean whatever the mean is say at zero here you know which ones we could keep you know where is the most variance .raw data is there's no knowledge in the data other than the raw data ..99.9% of the data is in the testing andyou know if you do stupid back off or not maybe some other choices you made along along along the way those are the parameters . thenwhat I want to do in the rest of the class is talk about clustering and clustering . what are the cluster here says that theseclusters are stretched out in SK learn . clusters can be stretched out and stretched out . these algorithms are going to behave inthe metric is take the distance s the squares do the same thing for that take all the distances inside the Clusters and itif you think of this as a smooth curve the derivative is giving you how fast it's changing . so basically something like this

Lecture 31
if I stay on this side audio is better so I'll ask some about I tend to wander around a lot during the leuregovernment and news and editor will be closer it's a really interesting way to look at the data I'm a little suspicious ofwhere is from the mean whatever the mean is say at zero here you know which ones we could keep you know where is the most variance .raw data is there's no knowledge in the data other than the raw data ..99.9% of the data is in the testing andyou know if you do stupid back off or not maybe some other choices you made along along along the way those are the parameters . thenthe algorithm is a noral li . this is what we're going to do right . it's the standardit's trivial you find the mean of all the points you find . all the X dimensions or how many dimensions you have andthere's only one voting boo in the cluster defined by the closest Center . and then you set the cluster to be the mean andwe wanted the intercluster distances to be as small as possible . we're only focusing on the first one so you can

Lecture 32
we're going to use the npy random Library to randomize your data set . there's no reason to have themyou can't change try it it won't let you change it so just remember you know a unigram looks like that this works really well you don't have to check whether the key is in there before adding one to it this is going to cause an errorthis is the expansion and this is Slide Slide the window across but in the beginning the window is small the AC at this expansion from your point ofcheck when you do this division and you're using stupid backup check the nerator if it's zero just return . maybeit's up to you just in the example I mean in here you should always do it in case I don't think I used itthe car's far windshield panel turned into a silver web with a dark hole in the center period . it's got if anyone wants to do this for a project it's pretty fun I had a lot of fun with this smer you choose the best five and these nbers are parameters so you choose to best five one two three four five . we'll

Lecture 33
if you've already started the person that's fine it's a reasonable model it didn't follow what I did intwo vectors are two characters to sentences to whatever it is that you're modeling when the cosine is is close to one okay whenif you wanna answer that so the total vocabulary size if we want to initialize characters in this movie in a vector space toif something like euclidean distance might be a little more I'm not sure how to interpret the angle it'if we do the exact same thing that I just did and this one is using term frequency in TD IDF with soft words remote itthe word is not enough specificity about me . it's a blunt force blunt instrent for doing the kind of thingsvalence of nouns car depends on how you feel about cars doesn't it arousal tuna is tuna 

Lecture 34
if you've already started the person that's fine it's a reasonable model it didn't follow what I did intwo vectors are two characters to sentences to whatever it is that you're modeling when the cosine is is close to one okay whenthis is unique works so the vocabulary here is 1249 words spoken by those eight characters . if you wanna answer that so the totalif something like euclidean distance might be a little more I'm not sure how to interpret the angle it'if we do the exact same thing that I just did and this one is using term frequency in TD IDF with soft words remote itthe word is not enough specificity about me . it's a blunt force blunt instrent for doing the kind of thingsvalence of nouns car depends on how you feel about cars doesn't it arousal tuna is tuna 

Lecture 35
if you don't want to do this I'm trying to get classical texts . sometimes you have this one is a transcriptionwhat we're going to do is build this the next time built one of these . you'll have fun with it you'rewe're going to carry the next word following the distribution of that diagram that would complete it . sample from the distribution . of fiveJohn likes to play cards now there's a sentence that wasn't there right this can generate infinite nber of sentences itwe're gonna the brown Corpus is divide you can if you look at it details . it's a whole bunchthe first idea of a language probabilistic language model is to assign a probability to Strings attention of words and a side benefit notthe formula remembers low perplexity bad what is it in Orwell's animal farm two legs bad two legs good four . probabilitythe probabilities are zero now what you want to do is smooth this . you add one to every count now they're not zero 

Lecture 36
if you don't want to do this I'm trying to get classical texts . sometimes you have this one is a transcriptionwhat we're going to do is build this the next time built one of these . you'll have fun with it you'rewe're going to carry the next word following the distribution of that diagram that would complete it . sample from the distribution . of fivethe script you know Pirates has Loops so it has an infinite nber of sentences . it's enough for a pirateextrinsic outside evaluation uses information that is outside the model . blah is a bigram model and a triinverse of probability the perplexity metric in natural language is right out of this article is a way to capture the degree of uncertaintythe first character I'm just going to give you the intuition it shows reported by the formula if I say to you . it'it works pretty well in text classification and it works very well . [Music] show up in weird places because sometimes and you can always tell

Lecture 37
foreign and been working on the assignment along with it a travel schedule and everything so what I want to do is hand out to Friday when Iit's all about how do you calculate the probability of a bigram trigram that you've never seen before . adding oneyou could run the system and adjust those ways parameters . you can also learn them talk about learning them . the weights you'rewe're going to compare term frequency I'm going to keep going . we want to use term frequency vectors for things like thisthe angle we want to use the angle between the two here's Wayne plays Beethoven Symphonies on the Vermont . it'sit's hard to find something that's opposite if if you had a term follow the term frequency Vector because frequencies have tocan you replace this with the new line oh this is the capture groups when you have a raw strength it has these Roofing it

Lecture 38
foreign and been working on the assignment along with it a travel schedule and everything so what I want to do is hand out to Friday when Iit's all about how do you calculate the probability of a bigram trigram that you've never seen before . adding oneyou could run the system and adjust those ways parameters . you can also learn them talk about learning them . the weights you'rethis is a slightly different question I'm in a particular place in the docent space . there's no obvious way tothe angle we want to use the angle between the two here's Wayne plays Beethoven Symphonies on the Vermont . it'sthe query builds a bad word the term frequency vector . you've got to find the docent that's close to the querythis is the capture groups when you have a raw strength it has these Roofing it will bind this to a variable you want to

Lecture 39
we're going to combine the text with the bag of words . if you want to have some experience with the wranglingyou can look at various amounts of it if you take this off you're going to see the whole thing I'll tell you inthe character is the beginning of the string so here it only replaces the last one . the dollar sign is the end of so control optionthere's got to be two blanks if there had been blanks here you know . right you want to keep the word separateit matches any decimal digit this stands for character class zero one two three four five six seven eight nine s stands for white space characterspirates make sure you're referring to the right care for the right string again let me just do 2000 characters . if Iso you don't have to be exhaustive you're trying to come up with I mean I want you to be as exhausted as you cana character actor said it so they couldn't figure out who was saying it . we're only interested in two characters but lookif you're going to expand this to who he is then delete the is you might as well just delete that and if itif you look up something that's not in the dictionary it returns not present what we want to do is return a zero .Wayne Snyder is occurring at the beginning of the line . in order to just not have another thing that you know to use this in but justput a Mark here you know put them a comments explain what you're thinking about all if you want to do a really

Lecture 40
this side of the room is five seconds ahead of this side relativistic effect . but I really want to look it over that's awesomethe maxim length of the word in the brown corpus was 33 characters long . it's wasteful and even worse the representation doesn'we're going to look at models obviously that don't do this . the vocabulary of the messages that you put on Piazzahere's an obvious question what's the relationship between bag of words suppose I had the one hot encodings of all the wordsyou'll have different words the same but you're in different places here in multiple places . that's why we try to keepyou know you just don't have enough data in the infinite set of all possible sentences and all possible texts you would have an infinite na sentence with people on the wall rides had Revisited the first sentence I've been here before first Sebastian women how could yougoogle stopped this in 2019 moved on other projects but you can look up engrams . the most likely thing to follow the boy istheme search you don't want to just generate any sentence you want to generate the most likely sentence in a particular context . what wewe're going to use these things to the neural network so they just changed the algorithms so it can take addiction it's basically in default

Lecture 41
python refresher focuses on a few things that you know it's up to you . if you'so it's going to say well three occurs five times one two three four five five four occurs etc etc it builds a dictionary from the entire review would have multiple paragraphs we have sentences . the level of characters it's not provided to you you can't sayif you don't want H and lowercase H to be the same, you can print out a bar chart of the percentagesyou'll figure it out so we don't want to be different uppercase from you know the lower case so make everything and and storelist of words filter them using a condition and only allow the ones that are normal words then go through and use your counter in the same old

Lecture 42
this is a first video in using python intended for students in 237 and in 1 132 . this one isyou are updating that as you go along most of the problems with python at least that i have to do with this globalthis is the location of index one this is two it goes from zero to one less than the size just like in an array in java k in range length of l is going to simulate the for loop that goes from the beginning to the end . so it stops sothe more i do this it just keeps adding this .sorted get x make it sorted the list you can also sort it thisthe key hash 23 put it in the location see there it is it's a hash table . it means sometimes you can

Lecture 43
the ipanaconda will show you all the same files that you had in you know it's the same as the fileyou can run all run if you're here and you want to run from here downward . you can select run all below the otherthe most important thing to remember is you have to surround the expression which you want to be mathematically typeset in dollar signs there's twoi put double double dollar signs and look i get this nice thing this would be the same you can use the frack and binomif there were no repeated letters it'd be 13 factorial but because of the repeated letters right there's four s'swe want to choose a random nber between zero and one based on this right . the x is going to be frompiazza hasn't been graded so you can't see the results . if you have problems and have fun our

Lecture 44
the subtitle of this notebook is how to write beautiful answers in your 237 homeworks . if you want to create cells in the notebookunformatted below shows you how it gets interpreted . list item dedered list works pretty well if you're familiar withi will use preview and i open it up in preview . let me go back share the whole screen . save it you got

Lecture 45
this is the file from the website and as i've said to you i keep this open . it's going to justthis is available in many languages like haskell and okamel . the for loop is here and the way to read this iswhen i ran this again it looked this up and said oh that's a list of variables it says list object is notthe rest of this folks are basically what i did in the first assignment . i would ask you to look down here and think about

